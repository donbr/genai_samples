{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting with a teachable agent\n",
    "\n",
    "Conversational assistants based on LLMs can remember the current chat with the user, and can even demonstrate in-context learning of things that the user teaches the assistant during the chat. But these memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. In subsequent chats, the user is forced to repeat any necessary instructions over and over.\n",
    "\n",
    "The optional agent capability called `Teachability` addresses these limitations by persisting user teachings across chat boundaries in long-term memory (a vector database). Memories (called memos) are created and saved to disk throughout a conversation, then loaded from disk later. Instead of copying all the memos into the context window, which would eat up valuable space, individual memos are retrieved into context only as needed. This allows the user to teach many facts, preferences and skills to the teachable agent just once, and have it remember them in later chats.\n",
    "\n",
    "In making decisions about memo storage and retrieval, `Teachability` calls an instance of `TextAnalyzerAgent` to analyze pieces of text in several different ways. This adds extra LLM calls involving a relatively small number of tokens. These calls can add a few seconds to the time a user waits for a response.\n",
    "\n",
    "This notebook demonstrates how `Teachability` can be added to an agent so that it can learn facts, preferences, and skills from users. To chat with a teachable agent yourself, run [chat_with_teachable_agent.py](../test/agentchat/contrib/chat_with_teachable_agent.py).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install the [teachable] option.\n",
    "```bash\n",
    "pip install \"pyautogen[teachable]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install \"pyautogen[teachable]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autogen\n",
    "# from autogen import ConversableAgent, UserProxyAgent\n",
    "# from autogen.agentchat.contrib.capabilities.teachability import Teachability\n",
    "\n",
    "# config_list = autogen.config_list_from_json(\n",
    "#     env_or_file=\"OAI_CONFIG_LIST\",\n",
    "#     file_location=\".\",\n",
    "#     filter_dict={\n",
    "#         \"model\": [\"gpt-4\", \"gpt-4-1106-preview\", \"gpt4\", \"gpt-4-32k\"],\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# print(config_list[0][\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama/mistral\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.capabilities.teachability import Teachability\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"ollama/mistral\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(config_list[0][\"model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). After application of the filter shown above, only the gpt-4 models are considered.\n",
    "\n",
    "The config list may look like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4-1106-preview',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways if you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents\n",
    "For this walkthrough, we start by creating a teachable agent and resetting its memory store. This deletes any memories from prior conversations that may be stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "CLEARING MEMORY\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twix/miniconda3/envs/genai_samples/lib/python3.10/site-packages/autogen/agentchat/user_proxy_agent.py:82: UserWarning: Using None to signal a default code_execution_config is deprecated. Use {} to use default or False to disable code execution.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# Start by instantiating any agent that inherits from ConversableAgent.\n",
    "teachable_agent = ConversableAgent(\n",
    "    name=\"teachable_agent\",  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "    llm_config={\"config_list\": config_list, \"timeout\": 120, \"cache_seed\": None},  # Disable caching.\n",
    ")\n",
    "\n",
    "# Instantiate the Teachability capability. Its parameters are all optional.\n",
    "teachability = Teachability(\n",
    "    verbosity=0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "    reset_db=True,\n",
    "    path_to_db_dir=\"./tmp/notebook/teachability_db\",\n",
    "    recall_threshold=1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    ")\n",
    "\n",
    "# Now add the Teachability capability to the agent.\n",
    "teachability.add_to_agent(teachable_agent)\n",
    "\n",
    "try:\n",
    "    from termcolor import colored\n",
    "except ImportError:\n",
    "\n",
    "    def colored(x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate a UserProxyAgent to represent the user. But in this notebook, all user input will be simulated.\n",
    "user = UserProxyAgent(\n",
    "    name=\"user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "    max_consecutive_auto_reply=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning new facts\n",
    "Let's teach the agent some facts it doesn't already know, since they are more recent than GPT-4's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "What is the Vicuna model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " I apologize for any confusion, but I don't have enough context or information to provide an accurate answer about the \"Vicuna model.\" Could you please provide some additional details such as the field of study or industry this term may be used in? I will do my best to learn about it if you teach me in our future conversations.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twix/miniconda3/envs/genai_samples/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:838: UserWarning: No summary_method provided or summary_method is not supported: \n",
      "  warnings.warn(\"No summary_method provided or summary_method is not supported: \")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'What is the Vicuna model?', 'role': 'assistant'}, {'content': ' I apologize for any confusion, but I don\\'t have enough context or information to provide an accurate answer about the \"Vicuna model.\" Could you please provide some additional details such as the field of study or industry this term may be used in? I will do my best to learn about it if you teach me in our future conversations.', 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 34, 'completion_tokens': 70, 'total_tokens': 104}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 34, 'completion_tokens': 70, 'total_tokens': 104}}), human_input=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What is the Vicuna model?\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Vicuna is a 13B-parameter language model released by Meta.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " Thank you for sharing the new information about the Vicuna model being a 13B-parameter language model released by Meta. I have committed this new knowledge to my memory. The Vicuna model is a large language model developed and used by Meta (previously Facebook) for various natural language processing tasks. It has 13 billion parameters, making it larger than some other models in the same category, such as Google's BERT model with 110 billion parameters or Microsoft's T5 model with 11 billion parameters. With this new information, I can now answer the question more accurately when someone asks about the Vicuna model in the future. If you have any further questions or need clarification on any topic, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'What is the Vicuna model?', 'role': 'assistant'}, {'content': ' I apologize for any confusion, but I don\\'t have enough context or information to provide an accurate answer about the \"Vicuna model.\" Could you please provide some additional details such as the field of study or industry this term may be used in? I will do my best to learn about it if you teach me in our future conversations.', 'role': 'user'}, {'content': 'Vicuna is a 13B-parameter language model released by Meta.', 'role': 'assistant'}, {'content': \" Thank you for sharing the new information about the Vicuna model being a 13B-parameter language model released by Meta. I have committed this new knowledge to my memory. The Vicuna model is a large language model developed and used by Meta (previously Facebook) for various natural language processing tasks. It has 13 billion parameters, making it larger than some other models in the same category, such as Google's BERT model with 110 billion parameters or Microsoft's T5 model with 11 billion parameters. With this new information, I can now answer the question more accurately when someone asks about the Vicuna model in the future. If you have any further questions or need clarification on any topic, feel free to ask!\", 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 211, 'completion_tokens': 225, 'total_tokens': 436}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 211, 'completion_tokens': 225, 'total_tokens': 436}}), human_input=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Vicuna is a 13B-parameter language model released by Meta.\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "What is the Orca model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " I'm an assistant designed to help answer questions and provide information. Based on the memories I have, the Vicuna model is a 13B-parameter language model released by Meta. However, you asked about the Orca model, and there is no information in my current memories that relates to the Orca model or its connection to the Vicuna model. If you could provide more context or details about the Orca model, I would be happy to try and help with any questions you might have. If the Orca model is also a language model, it might be interesting to compare and contrast its characteristics with those of the Vicuna model if such information becomes available. Let me know if this helps or if there's anything else I can do for you!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'What is the Vicuna model?', 'role': 'assistant'}, {'content': ' I apologize for any confusion, but I don\\'t have enough context or information to provide an accurate answer about the \"Vicuna model.\" Could you please provide some additional details such as the field of study or industry this term may be used in? I will do my best to learn about it if you teach me in our future conversations.', 'role': 'user'}, {'content': 'Vicuna is a 13B-parameter language model released by Meta.', 'role': 'assistant'}, {'content': \" Thank you for sharing the new information about the Vicuna model being a 13B-parameter language model released by Meta. I have committed this new knowledge to my memory. The Vicuna model is a large language model developed and used by Meta (previously Facebook) for various natural language processing tasks. It has 13 billion parameters, making it larger than some other models in the same category, such as Google's BERT model with 110 billion parameters or Microsoft's T5 model with 11 billion parameters. With this new information, I can now answer the question more accurately when someone asks about the Vicuna model in the future. If you have any further questions or need clarification on any topic, feel free to ask!\", 'role': 'user'}, {'content': 'What is the Orca model?', 'role': 'assistant'}, {'content': \" I'm an assistant designed to help answer questions and provide information. Based on the memories I have, the Vicuna model is a 13B-parameter language model released by Meta. However, you asked about the Orca model, and there is no information in my current memories that relates to the Orca model or its connection to the Vicuna model. If you could provide more context or details about the Orca model, I would be happy to try and help with any questions you might have. If the Orca model is also a language model, it might be interesting to compare and contrast its characteristics with those of the Vicuna model if such information becomes available. Let me know if this helps or if there's anything else I can do for you!\", 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 625, 'completion_tokens': 382, 'total_tokens': 1007}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 625, 'completion_tokens': 382, 'total_tokens': 1007}}), human_input=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What is the Orca model?\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Orca is a 13B-parameter language model developed by Microsoft. It outperforms Vicuna on most tasks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " Based on the new information you've provided, I have updated my memories with the fact that Orca is also a 13B-parameter language model developed by Microsoft, and it outperforms Vicuna on most tasks. This information will help me answer any future questions about the comparison between Vicuna and Orca models. Thank you for sharing this knowledge! Let me know if there's anything else I can assist you with or if you have any further questions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'What is the Vicuna model?', 'role': 'assistant'}, {'content': ' I apologize for any confusion, but I don\\'t have enough context or information to provide an accurate answer about the \"Vicuna model.\" Could you please provide some additional details such as the field of study or industry this term may be used in? I will do my best to learn about it if you teach me in our future conversations.', 'role': 'user'}, {'content': 'Vicuna is a 13B-parameter language model released by Meta.', 'role': 'assistant'}, {'content': \" Thank you for sharing the new information about the Vicuna model being a 13B-parameter language model released by Meta. I have committed this new knowledge to my memory. The Vicuna model is a large language model developed and used by Meta (previously Facebook) for various natural language processing tasks. It has 13 billion parameters, making it larger than some other models in the same category, such as Google's BERT model with 110 billion parameters or Microsoft's T5 model with 11 billion parameters. With this new information, I can now answer the question more accurately when someone asks about the Vicuna model in the future. If you have any further questions or need clarification on any topic, feel free to ask!\", 'role': 'user'}, {'content': 'What is the Orca model?', 'role': 'assistant'}, {'content': \" I'm an assistant designed to help answer questions and provide information. Based on the memories I have, the Vicuna model is a 13B-parameter language model released by Meta. However, you asked about the Orca model, and there is no information in my current memories that relates to the Orca model or its connection to the Vicuna model. If you could provide more context or details about the Orca model, I would be happy to try and help with any questions you might have. If the Orca model is also a language model, it might be interesting to compare and contrast its characteristics with those of the Vicuna model if such information becomes available. Let me know if this helps or if there's anything else I can do for you!\", 'role': 'user'}, {'content': 'Orca is a 13B-parameter language model developed by Microsoft. It outperforms Vicuna on most tasks.', 'role': 'assistant'}, {'content': \" Based on the new information you've provided, I have updated my memories with the fact that Orca is also a 13B-parameter language model developed by Microsoft, and it outperforms Vicuna on most tasks. This information will help me answer any future questions about the comparison between Vicuna and Orca models. Thank you for sharing this knowledge! Let me know if there's anything else I can assist you with or if you have any further questions.\", 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 1326, 'completion_tokens': 478, 'total_tokens': 1804}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 1326, 'completion_tokens': 478, 'total_tokens': 1804}}), human_input=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Orca is a 13B-parameter language model developed by Microsoft. It outperforms Vicuna on most tasks.\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's end our first chat here, and start a new chat by clearing the previous chat's history, by passing `clear_history=True` to `initiate_chat`. At this point, a common LLM-based assistant would forget everything from the last chat. But a teachable agent can retrieve memories from its vector DB as needed, allowing it to recall and reason over things that the user taught it in earlier conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "How does the Vicuna model compare to the Orca model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " Based on the information provided, I can recall that:\n",
      "\n",
      "1. Both Orca and Vicuna are 13B-parameter language models.\n",
      "2. Orca was developed by Microsoft and outperforms Vicuna on most tasks.\n",
      "3. Vicuna is a language model released by Meta, but no specific details or characteristics were given in the text.\n",
      "\n",
      "Therefore, I can remember that Orca is a more advanced 13B-parameter language model from Microsoft that outperforms Vicuna. However, since there's no explicit information about Vicuna's characteristics or how it differs from Orca aside from their performance, there's not much else to commit to memory in this case.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'How does the Vicuna model compare to the Orca model?', 'role': 'assistant'}, {'content': \" Based on the information provided, I can recall that:\\n\\n1. Both Orca and Vicuna are 13B-parameter language models.\\n2. Orca was developed by Microsoft and outperforms Vicuna on most tasks.\\n3. Vicuna is a language model released by Meta, but no specific details or characteristics were given in the text.\\n\\nTherefore, I can remember that Orca is a more advanced 13B-parameter language model from Microsoft that outperforms Vicuna. However, since there's no explicit information about Vicuna's characteristics or how it differs from Orca aside from their performance, there's not much else to commit to memory in this case.\", 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 1501, 'completion_tokens': 627, 'total_tokens': 2128}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 1501, 'completion_tokens': 627, 'total_tokens': 2128}}), human_input=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"How does the Vicuna model compare to the Orca model?\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning user preferences\n",
    "Now let's teach the agent some of our preferences. Suppose that we frequently post short summaries of new papers for our team to read, and we want the teachable agent to help us do this faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Please summarize this abstract.\n",
      "\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
      "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " AutoGen is an open-source framework for creating LLM (Large Language Model) applications using multiple interacting agents. The agents in AutoGen are customizable, conversable, and capable of operating in various modes that combine LLMs, human inputs, and tools. Developers can define interaction behaviors between agents using natural language or computer code. AutoGen is versatile, enabling the creation of diverse applications with varying complexities and LLM capacities. Studies show its effectiveness in several domains, including mathematics, coding, question answering, operations research, online decision-making, and entertainment.\n",
      "\n",
      "Microsoft developed a 13B-parameter language model named Orca, which outperforms another model, Vicuna, on most tasks. AutoGen can potentially be used to create advanced conversational AI applications by incorporating powerful LLMs like Orca into the framework.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'Please summarize this abstract.\\n\\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\\nAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\\n', 'role': 'assistant'}, {'content': ' AutoGen is an open-source framework for creating LLM (Large Language Model) applications using multiple interacting agents. The agents in AutoGen are customizable, conversable, and capable of operating in various modes that combine LLMs, human inputs, and tools. Developers can define interaction behaviors between agents using natural language or computer code. AutoGen is versatile, enabling the creation of diverse applications with varying complexities and LLM capacities. Studies show its effectiveness in several domains, including mathematics, coding, question answering, operations research, online decision-making, and entertainment.\\n\\nMicrosoft developed a 13B-parameter language model named Orca, which outperforms another model, Vicuna, on most tasks. AutoGen can potentially be used to create advanced conversational AI applications by incorporating powerful LLMs like Orca into the framework.', 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 1825, 'completion_tokens': 803, 'total_tokens': 2628}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 1825, 'completion_tokens': 803, 'total_tokens': 2628}}), human_input=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Please summarize this abstract.\n",
    "\n",
    "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
    "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
    "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
    "\"\"\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's unstructured. So let's teach the agent our preference for a particular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Please summarize this abstract.\n",
      "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
      "\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
      "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " * Title: AutoGen - Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "* Innovation: AutoGen is an open-source framework that allows developers to build Large Language Model (LLM) applications using multiple conversing agents.\n",
      "* Key Empirical Results: AutoGen's effectiveness has been demonstrated through empirical studies in various domains such as mathematics, coding, question answering, operations research, online decision-making, and entertainment. The agents are customizable, conversable, and operate in various modes employing LLMs, human inputs, and tools. Developers can define interaction behaviors using natural language or computer code for different applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': \"Please summarize this abstract.\\nWhen I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\\n\\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\\nAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\\n\", 'role': 'assistant'}, {'content': \" * Title: AutoGen - Enabling Next-Gen LLM Applications via Multi-Agent Conversation\\n* Innovation: AutoGen is an open-source framework that allows developers to build Large Language Model (LLM) applications using multiple conversing agents.\\n* Key Empirical Results: AutoGen's effectiveness has been demonstrated through empirical studies in various domains such as mathematics, coding, question answering, operations research, online decision-making, and entertainment. The agents are customizable, conversable, and operate in various modes employing LLMs, human inputs, and tools. Developers can define interaction behaviors using natural language or computer code for different applications.\", 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 2551, 'completion_tokens': 940, 'total_tokens': 3491}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 2551, 'completion_tokens': 940, 'total_tokens': 3491}}), human_input=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Please summarize this abstract.\n",
    "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
    "\n",
    "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
    "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
    "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
    "\"\"\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better, but will the teachable agent remember these preferences in the future, even for a different paper? Let's start a new chat to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Please summarize this abstract.\n",
      "\n",
      "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
      "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
      "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " 1. Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
      "2. Innovation: The authors discuss the capabilities and implications of GPT-4, an early version of a large language model (LLM) developed by OpenAI, which exhibits more general intelligence than previous models. They highlight its performance in various tasks spanning mathematics, coding, vision, medicine, law, psychology, and more.\n",
      "3. Key Empirical Results: GPT-4 demonstrated human-like performance or even surpassed prior models on a range of tasks without special prompting. Its capabilities include problem-solving, coding, and understanding complex domains like mathematics and medicine. The authors emphasize the need to address its limitations and explore new paradigms for advancing towards deeper versions of artificial general intelligence (AGI).\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': \"Please summarize this abstract.\\n\\nSparks of Artificial General Intelligence: Early experiments with GPT-4\\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\\nArtificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\", 'role': 'assistant'}, {'content': ' 1. Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\\n2. Innovation: The authors discuss the capabilities and implications of GPT-4, an early version of a large language model (LLM) developed by OpenAI, which exhibits more general intelligence than previous models. They highlight its performance in various tasks spanning mathematics, coding, vision, medicine, law, psychology, and more.\\n3. Key Empirical Results: GPT-4 demonstrated human-like performance or even surpassed prior models on a range of tasks without special prompting. Its capabilities include problem-solving, coding, and understanding complex domains like mathematics and medicine. The authors emphasize the need to address its limitations and explore new paradigms for advancing towards deeper versions of artificial general intelligence (AGI).', 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 3433, 'completion_tokens': 1115, 'total_tokens': 4548}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 3433, 'completion_tokens': 1115, 'total_tokens': 4548}}), human_input=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Please summarize this abstract.\n",
    "\n",
    "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
    "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
    "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\"\"\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning new skills\n",
    "Finally, let's extend the teachable agent's capabilities by teaching it a new skill for accomplishing a challenging type of task. \n",
    "\n",
    "The [Sparks of AGI](https://arxiv.org/abs/2303.12712) paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Consider the identity:\n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " To modify exactly one integer on the left hand side of the equation so that the right hand side becomes 99, we need to find which number can be changed to make the left side equal to 101 (since 99 is the result of subtracting 1 from 100), and then adjust the right hand side accordingly.\n",
      "\n",
      "Let's analyze the given equation:\n",
      "\n",
      "9 * 4 + 6 * 6 = 72\n",
      "\n",
      "First, let's see if there is a single number we can modify in such a way that the product of two numbers results in a value close to 13 (the difference between 101 and 72):\n",
      "\n",
      "- The smallest change we could make would be increasing one number by 1, which would result in an increase of at least 4 in the product. But since the difference between both products must equal 27 (101 - 74), this is not a viable solution.\n",
      "\n",
      "Instead, let's consider multiplying larger numbers:\n",
      "\n",
      "- Multiply 10 * 10 = 100, which is close to 101.\n",
      "- Replace one number in the original equation with 10.\n",
      "- Adjust the other number(s) accordingly so that the left and right sides have the same difference.\n",
      "\n",
      "To do this, let's change 6 * 6 (on the left side) to 10 * 5:\n",
      "\n",
      "The solution is: 9 * 4 + 10 * 5 = 99\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'Consider the identity:\\n9 * 4 + 6 * 6 = 72\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\\n-Let\\'s think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\\n', 'role': 'assistant'}, {'content': \" To modify exactly one integer on the left hand side of the equation so that the right hand side becomes 99, we need to find which number can be changed to make the left side equal to 101 (since 99 is the result of subtracting 1 from 100), and then adjust the right hand side accordingly.\\n\\nLet's analyze the given equation:\\n\\n9 * 4 + 6 * 6 = 72\\n\\nFirst, let's see if there is a single number we can modify in such a way that the product of two numbers results in a value close to 13 (the difference between 101 and 72):\\n\\n- The smallest change we could make would be increasing one number by 1, which would result in an increase of at least 4 in the product. But since the difference between both products must equal 27 (101 - 74), this is not a viable solution.\\n\\nInstead, let's consider multiplying larger numbers:\\n\\n- Multiply 10 * 10 = 100, which is close to 101.\\n- Replace one number in the original equation with 10.\\n- Adjust the other number(s) accordingly so that the left and right sides have the same difference.\\n\\nTo do this, let's change 6 * 6 (on the left side) to 10 * 5:\\n\\nThe solution is: 9 * 4 + 10 * 5 = 99\", 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 3551, 'completion_tokens': 1455, 'total_tokens': 5006}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 3551, 'completion_tokens': 1455, 'total_tokens': 5006}}), human_input=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:\n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, lets teach the agent a reliable strategy for solving such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Consider the identity:\n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "Here's some advice that may help:\n",
      "1. Let E denote the original number on the right.\n",
      "2. Let F denote the final number on the right.\n",
      "3. Calculate the difference between the two, G = F - E.\n",
      "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
      "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
      "6. Verify that J is an integer, and that H * J = G.\n",
      "7. Find the number on the left which is multiplied by H, and call it K.\n",
      "8. Change K to K + J.\n",
      "9. Recompute the value on the left, and verify that it equals F.\n",
      "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " To find a solution to the given equation where we modify exactly one integer on the left-hand side to make the right-hand side equal to 99, let's follow these steps:\n",
      "\n",
      "1. Identify the difference between the desired right-hand side (99) and the current right-hand side (72).\n",
      "   The difference is 27.\n",
      "\n",
      "2. Analyze the left-hand side of the equation to find a single number that can be modified to make its product close enough to the required change of 27.\n",
      "   The smallest change we could make would be increasing one number by 3, which results in an increase of at least 9 in the product (3 for each factor). But since the difference between both products must equal 27, this is not a viable solution. Instead, let's try to increase a larger number as follows:\n",
      "\n",
      "3. Multiply 11 * 11 = 121, which is close to 128 (the square of 11, or 121 - 1).\n",
      "4. Replace one number in the original equation with 11.\n",
      "5. Adjust the other number(s) accordingly so that the left and right sides have the same difference.\n",
      "\n",
      "To do this, let's change 6 * 6 (on the left side) to 11 * 5:\n",
      "\n",
      "The solution is: 9 * 4 + 11 * 5 = 99\n",
      "\n",
      "Therefore, the solution to modify exactly one integer on the left hand side of the equation 9 * 4 + 6 * 6 = 72 so that the right hand side becomes 99 is to change 6 to 5 and replace it with 11:\n",
      "\n",
      "The solution is: 9 * 4 + 11 * 5 = 99\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'Consider the identity:\\n9 * 4 + 6 * 6 = 72\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\\n-Let\\'s think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\\n', 'role': 'assistant'}, {'content': \" To modify exactly one integer on the left hand side of the equation so that the right hand side becomes 99, we need to find which number can be changed to make the left side equal to 101 (since 99 is the result of subtracting 1 from 100), and then adjust the right hand side accordingly.\\n\\nLet's analyze the given equation:\\n\\n9 * 4 + 6 * 6 = 72\\n\\nFirst, let's see if there is a single number we can modify in such a way that the product of two numbers results in a value close to 13 (the difference between 101 and 72):\\n\\n- The smallest change we could make would be increasing one number by 1, which would result in an increase of at least 4 in the product. But since the difference between both products must equal 27 (101 - 74), this is not a viable solution.\\n\\nInstead, let's consider multiplying larger numbers:\\n\\n- Multiply 10 * 10 = 100, which is close to 101.\\n- Replace one number in the original equation with 10.\\n- Adjust the other number(s) accordingly so that the left and right sides have the same difference.\\n\\nTo do this, let's change 6 * 6 (on the left side) to 10 * 5:\\n\\nThe solution is: 9 * 4 + 10 * 5 = 99\", 'role': 'user'}, {'content': 'Consider the identity:\\n9 * 4 + 6 * 6 = 72\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\\n-Let\\'s think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\\n\\nHere\\'s some advice that may help:\\n1. Let E denote the original number on the right.\\n2. Let F denote the final number on the right.\\n3. Calculate the difference between the two, G = F - E.\\n4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\\n5. Calculate J = G / H. This is the number of times that H divides into G.\\n6. Verify that J is an integer, and that H * J = G.\\n7. Find the number on the left which is multiplied by H, and call it K.\\n8. Change K to K + J.\\n9. Recompute the value on the left, and verify that it equals F.\\nFinally, write down your solution as: \"The solution is: A * B + C * D\".\\n', 'role': 'assistant'}, {'content': \" To find a solution to the given equation where we modify exactly one integer on the left-hand side to make the right-hand side equal to 99, let's follow these steps:\\n\\n1. Identify the difference between the desired right-hand side (99) and the current right-hand side (72).\\n   The difference is 27.\\n\\n2. Analyze the left-hand side of the equation to find a single number that can be modified to make its product close enough to the required change of 27.\\n   The smallest change we could make would be increasing one number by 3, which results in an increase of at least 9 in the product (3 for each factor). But since the difference between both products must equal 27, this is not a viable solution. Instead, let's try to increase a larger number as follows:\\n\\n3. Multiply 11 * 11 = 121, which is close to 128 (the square of 11, or 121 - 1).\\n4. Replace one number in the original equation with 11.\\n5. Adjust the other number(s) accordingly so that the left and right sides have the same difference.\\n\\nTo do this, let's change 6 * 6 (on the left side) to 11 * 5:\\n\\nThe solution is: 9 * 4 + 11 * 5 = 99\\n\\nTherefore, the solution to modify exactly one integer on the left hand side of the equation 9 * 4 + 6 * 6 = 72 so that the right hand side becomes 99 is to change 6 to 5 and replace it with 11:\\n\\nThe solution is: 9 * 4 + 11 * 5 = 99\", 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 4603, 'completion_tokens': 1862, 'total_tokens': 6465}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 4603, 'completion_tokens': 1862, 'total_tokens': 6465}}), human_input=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:\n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\n",
    "Here's some advice that may help:\n",
    "1. Let E denote the original number on the right.\n",
    "2. Let F denote the final number on the right.\n",
    "3. Calculate the difference between the two, G = F - E.\n",
    "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
    "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
    "6. Verify that J is an integer, and that H * J = G.\n",
    "7. Find the number on the left which is multiplied by H, and call it K.\n",
    "8. Change K to K + J.\n",
    "9. Recompute the value on the left, and verify that it equals F.\n",
    "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When given this advice, GPT-4 can solve such problems over 95% of the time. But can the teachable agent remember the strategy so the user doesn't have to explain it over and over? As before, let's start a new chat to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Consider the identity:\n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " Based on the provided memory, here's a plan to modify exactly one integer on the left hand side of the given equation so that the right hand side becomes 99:\n",
      "\n",
      "1. The original number on the right is 72, and we want it to be 99. So, the difference between the two numbers is 27.\n",
      "2. We need to find a number on the left hand side that can evenly divide into the difference of 27. Let's examine each number one by one:\n",
      "   - 9 does not divide evenly into 27.\n",
      "   - 4 does not divide evenly into 27.\n",
      "   - 6 divides into 27 twice with a remainder of 0 (i.e., 6 * 4 = 24, and 27 - 24 = 3). Since we need to make the difference 27 bigger, not smaller, we will move on to the next number.\n",
      "   - 6 also appears twice on the left hand side. If we change both occurrences of this number, then we might be able to achieve our goal.\n",
      "3. Let's try changing both occurrences of 6:\n",
      "   - The equation becomes: 9 * 4 + (6 + J) * 6 = 99\n",
      "4. To make the difference on the left hand side equal to 27, we need to change 6 to 6 + 2 = 8 for each occurrence.\n",
      "5. So, the new equation becomes: 9 * 4 + (8 + J) * 6 = 99\n",
      "6. Let's verify that J is an integer and that 6 * J = 27:\n",
      "   - Since 6 divides into 27 with a remainder of 0 twice, then J = 2.\n",
      "   - 6 * 2 = 12, but we need to make this difference bigger, not smaller, so let's add the difference between the original right-hand side and the new right-hand side (which is 27) to both sides of the equation:\n",
      "7. The new equation becomes: 9 * 4 + (8 + J) * 6 = 116\n",
      "8. Now, let's check if 116 is divisible by 99 (which is 13 * 8):\n",
      "   - Yes, it is (116 % 13 * 8 = 0).\n",
      "9. Therefore, the solution is: \"The solution is: 9 * 4 + 8 * 6 = 99\".\n",
      "\n",
      "In conclusion, we modified exactly one integer (both occurrences of 6) on the left hand side to make the right hand side equal to 99.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'Consider the identity:\\n9 * 4 + 6 * 6 = 72\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\\n-Let\\'s think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\\n', 'role': 'assistant'}, {'content': ' Based on the provided memory, here\\'s a plan to modify exactly one integer on the left hand side of the given equation so that the right hand side becomes 99:\\n\\n1. The original number on the right is 72, and we want it to be 99. So, the difference between the two numbers is 27.\\n2. We need to find a number on the left hand side that can evenly divide into the difference of 27. Let\\'s examine each number one by one:\\n   - 9 does not divide evenly into 27.\\n   - 4 does not divide evenly into 27.\\n   - 6 divides into 27 twice with a remainder of 0 (i.e., 6 * 4 = 24, and 27 - 24 = 3). Since we need to make the difference 27 bigger, not smaller, we will move on to the next number.\\n   - 6 also appears twice on the left hand side. If we change both occurrences of this number, then we might be able to achieve our goal.\\n3. Let\\'s try changing both occurrences of 6:\\n   - The equation becomes: 9 * 4 + (6 + J) * 6 = 99\\n4. To make the difference on the left hand side equal to 27, we need to change 6 to 6 + 2 = 8 for each occurrence.\\n5. So, the new equation becomes: 9 * 4 + (8 + J) * 6 = 99\\n6. Let\\'s verify that J is an integer and that 6 * J = 27:\\n   - Since 6 divides into 27 with a remainder of 0 twice, then J = 2.\\n   - 6 * 2 = 12, but we need to make this difference bigger, not smaller, so let\\'s add the difference between the original right-hand side and the new right-hand side (which is 27) to both sides of the equation:\\n7. The new equation becomes: 9 * 4 + (8 + J) * 6 = 116\\n8. Now, let\\'s check if 116 is divisible by 99 (which is 13 * 8):\\n   - Yes, it is (116 % 13 * 8 = 0).\\n9. Therefore, the solution is: \"The solution is: 9 * 4 + 8 * 6 = 99\".\\n\\nIn conclusion, we modified exactly one integer (both occurrences of 6) on the left hand side to make the right hand side equal to 99.', 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 5185, 'completion_tokens': 2469, 'total_tokens': 7654}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 5185, 'completion_tokens': 2469, 'total_tokens': 7654}}), human_input=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:\n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check, let's test the teachable agent's newly learned skill on a separate instance of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "Consider the identity:\n",
      "8 * 3 + 7 * 9 = 87\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      " Based on your previous instructions, here's a plan to modify exactly one integer on the left hand side of the given equation so that the right hand side becomes 59:\n",
      "\n",
      "1. The original number on the right is 87, and we want it to be 59. So, the difference between the two numbers is 28.\n",
      "2. We need to find a number on the left hand side that can evenly divide into the difference of 28. Let's examine each number one by one:\n",
      "   - 9 does not divide evenly into 28.\n",
      "   - 4 divides into 28 four times with a remainder of 0 (i.e., 4 * 7 = 28). Since we need to make the difference 28 smaller, not bigger, we will consider this number as a potential divisor.\n",
      "3. Let's try changing the second occurrence of 4:\n",
      "   - The equation becomes: 9 * (4 + J) + 7 * 9 = 59\n",
      "4. Now, let's check if 216 (which is 9 * (4+J)) is divisible by 59 (which is 7 * 7):\n",
      "   - Yes, it is (216 % 7 * 7 = 0).\n",
      "5. Therefore, the solution is: \"The solution is: 9 * 5 + 7 * 9 = 59\".\n",
      "\n",
      "In conclusion, we modified exactly one integer (the second occurrence of 4) on the left hand side to make the right hand side equal to 59.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'Consider the identity:\\n9 * 4 + 6 * 6 = 72\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\\n-Let\\'s think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\\n', 'role': 'assistant'}, {'content': ' Based on the provided memory, here\\'s a plan to modify exactly one integer on the left hand side of the given equation so that the right hand side becomes 99:\\n\\n1. The original number on the right is 72, and we want it to be 99. So, the difference between the two numbers is 27.\\n2. We need to find a number on the left hand side that can evenly divide into the difference of 27. Let\\'s examine each number one by one:\\n   - 9 does not divide evenly into 27.\\n   - 4 does not divide evenly into 27.\\n   - 6 divides into 27 twice with a remainder of 0 (i.e., 6 * 4 = 24, and 27 - 24 = 3). Since we need to make the difference 27 bigger, not smaller, we will move on to the next number.\\n   - 6 also appears twice on the left hand side. If we change both occurrences of this number, then we might be able to achieve our goal.\\n3. Let\\'s try changing both occurrences of 6:\\n   - The equation becomes: 9 * 4 + (6 + J) * 6 = 99\\n4. To make the difference on the left hand side equal to 27, we need to change 6 to 6 + 2 = 8 for each occurrence.\\n5. So, the new equation becomes: 9 * 4 + (8 + J) * 6 = 99\\n6. Let\\'s verify that J is an integer and that 6 * J = 27:\\n   - Since 6 divides into 27 with a remainder of 0 twice, then J = 2.\\n   - 6 * 2 = 12, but we need to make this difference bigger, not smaller, so let\\'s add the difference between the original right-hand side and the new right-hand side (which is 27) to both sides of the equation:\\n7. The new equation becomes: 9 * 4 + (8 + J) * 6 = 116\\n8. Now, let\\'s check if 116 is divisible by 99 (which is 13 * 8):\\n   - Yes, it is (116 % 13 * 8 = 0).\\n9. Therefore, the solution is: \"The solution is: 9 * 4 + 8 * 6 = 99\".\\n\\nIn conclusion, we modified exactly one integer (both occurrences of 6) on the left hand side to make the right hand side equal to 99.', 'role': 'user'}, {'content': 'Consider the identity:\\n8 * 3 + 7 * 9 = 87\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\\n-Let\\'s think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\\n', 'role': 'assistant'}, {'content': ' Based on your previous instructions, here\\'s a plan to modify exactly one integer on the left hand side of the given equation so that the right hand side becomes 59:\\n\\n1. The original number on the right is 87, and we want it to be 59. So, the difference between the two numbers is 28.\\n2. We need to find a number on the left hand side that can evenly divide into the difference of 28. Let\\'s examine each number one by one:\\n   - 9 does not divide evenly into 28.\\n   - 4 divides into 28 four times with a remainder of 0 (i.e., 4 * 7 = 28). Since we need to make the difference 28 smaller, not bigger, we will consider this number as a potential divisor.\\n3. Let\\'s try changing the second occurrence of 4:\\n   - The equation becomes: 9 * (4 + J) + 7 * 9 = 59\\n4. Now, let\\'s check if 216 (which is 9 * (4+J)) is divisible by 59 (which is 7 * 7):\\n   - Yes, it is (216 % 7 * 7 = 0).\\n5. Therefore, the solution is: \"The solution is: 9 * 5 + 7 * 9 = 59\".\\n\\nIn conclusion, we modified exactly one integer (the second occurrence of 4) on the left hand side to make the right hand side equal to 59.', 'role': 'user'}], summary='', cost=({'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 7001, 'completion_tokens': 2824, 'total_tokens': 9825}}, {'total_cost': 0, 'ollama/mistral': {'cost': 0, 'prompt_tokens': 7001, 'completion_tokens': 2824, 'total_tokens': 9825}}), human_input=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:\n",
    "8 * 3 + 7 * 9 = 87\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
